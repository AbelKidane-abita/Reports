{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-21 07:57:19.093061: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-21 07:57:19.769636: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-08-21 07:57:20.616450: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "Collecting en-core-web-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.9)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.28.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (68.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.5.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "2023-08-21 07:57:42.514128: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-21 07:57:43.126282: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-08-21 07:57:43.973516: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "Collecting de-core-news-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.6.0/de_core_news_sm-3.6.0-py3-none-any.whl (14.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from de-core-news-sm==3.6.0) (3.6.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.4.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.0.9)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.28.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (68.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2023.5.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.1.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (8.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.1.3)\n",
      "Installing collected packages: de-core-news-sm\n",
      "Successfully installed de-core-news-sm-3.6.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pip setuptools wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext    \n",
    "from torchtext.data import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torchtext\n",
      "Version: 0.15.2\n",
      "Summary: Text utilities and datasets for PyTorch\n",
      "Home-page: https://github.com/pytorch/text\n",
      "Author: PyTorch core devs and James Bradbury\n",
      "Author-email: jekbradbury@gmail.com\n",
      "License: BSD\n",
      "Location: /home/tr-abel/miniconda3/envs/env1/lib/python3.10/site-packages\n",
      "Requires: numpy, requests, torch, torchdata, tqdm\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'the',\n",
       " 'statement',\n",
       " 'that',\n",
       " 'I',\n",
       " 'want',\n",
       " 'to',\n",
       " 'tokenize',\n",
       " 'at',\n",
       " 'the',\n",
       " 'start',\n",
       " '!']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"This is the statement that I want to tokenize at the start!\"\n",
    "tokenizer = get_tokenizer(\"spacy\")\n",
    "\n",
    "My_tokens = tokenizer(text)\n",
    "\n",
    "My_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "am\n",
      "testing\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "sentence = \"I am testing\"\n",
    "for word in nltk.word_tokenize(sentence):\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchdata.datapipes as dp\n",
    "import torchtext.transforms as T\n",
    "import spacy\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "eng = spacy.load(\"en_core_web_sm\") # Load the English model to tokenize English text\n",
    "de = spacy.load(\"de_core_news_sm\") # Load the German model to tokenize German text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = 'deu.txt'  \n",
    "data_pipe = dp.iter.IterableWrapper([FILE_PATH])\n",
    "data_pipe = dp.iter.FileOpener(data_pipe, mode='rb')\n",
    "data_pipe = data_pipe.parse_csv(skip_lines=0, delimiter='\\t', as_tuple=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Go.', 'Geh.')\n",
      "('Hi.', 'Hallo!')\n",
      "('Hi.', 'Grüß Gott!')\n",
      "('Run!', 'Lauf!')\n",
      "('Run.', 'Lauf!')\n",
      "('Wow!', 'Potzdonner!')\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "for sample in data_pipe:\n",
    "    print(sample)\n",
    "    c+=1\n",
    "    if c>5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeAttribution(row):\n",
    "    \"\"\"\n",
    "    Function to keep the first two elements in a tuple\n",
    "    \"\"\"\n",
    "    return row[:2]\n",
    "data_pipe = data_pipe.map(removeAttribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engTokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenize an English text and return a list of tokens\n",
    "    \"\"\"\n",
    "    return [token.text for token in eng.tokenizer(text)]\n",
    "\n",
    "def deTokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenize a German text and return a list of tokens\n",
    "    \"\"\"\n",
    "    return [token.text for token in de.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = eng.tokenizer(\"My text is this one\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5\n",
      "8\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "for token in eng.tokenizer(\"This is my code\"):\n",
    "    print( token.idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Have', 'a', 'good', 'day', '!', '!', '!']\n",
      "['Haben', 'Sie', 'einen', 'guten', 'Tag', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "print(engTokenize(\"Have a good day!!!\"))\n",
    "print(deTokenize(\"Haben Sie einen guten Tag!!!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTokens(data_iter, place):\n",
    "    \"\"\"\n",
    "    Function to yield tokens from an iterator. Since, our iterator contains\n",
    "    tuple of sentences (source and target), `place` parameters defines for which\n",
    "    index to return the tokens for. `place=0` for source and `place=1` for target\n",
    "    \"\"\"\n",
    "    for english, german in data_iter:\n",
    "        if place == 0:\n",
    "            yield engTokenize(english)\n",
    "        else:\n",
    "            yield deTokenize(german)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_vocab = build_vocab_from_iterator(\n",
    "    getTokens(data_pipe,0),\n",
    "    min_freq=2,\n",
    "    specials= ['<pad>', '<sos>', '<eos>', '<unk>'],\n",
    "    special_first=True\n",
    ")\n",
    "source_vocab.set_default_index(source_vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vocab = build_vocab_from_iterator(\n",
    "    getTokens(data_pipe,1),\n",
    "    min_freq=2,\n",
    "    specials= ['<pad>', '<sos>', '<eos>', '<unk>'],\n",
    "    special_first=True\n",
    ")\n",
    "target_vocab.set_default_index(target_vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<sos>', '<eos>', '<unk>', '.', 'I', 'Tom', 'to', 'you']\n"
     ]
    }
   ],
   "source": [
    "print(source_vocab.get_itos()[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTransform(vocab):\n",
    "    \"\"\"\n",
    "    Create transforms based on given vocabulary. The returned transform is applied to sequence\n",
    "    of tokens.\n",
    "    \"\"\"\n",
    "    text_tranform = T.Sequential(\n",
    "        ## converts the sentences to indices based on given vocabulary\n",
    "        T.VocabTransform(vocab=vocab),\n",
    "        ## Add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is\n",
    "        # 1 as seen in previous section\n",
    "        T.AddToken(1, begin=True),\n",
    "        ## Add <eos> at beginning of each sentence. 2 because the index for <eos> in vocabulary is\n",
    "        # 2 as seen in previous section\n",
    "        T.AddToken(2, begin=False)\n",
    "    )\n",
    "    return text_tranform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some sentence=I giggled.\n",
      "Transformed sentence=[1, 5, 4894, 4, 2]\n",
      "<sos> I giggled . <eos> "
     ]
    }
   ],
   "source": [
    "temp_list = list(data_pipe)\n",
    "some_sentence = temp_list[798][0]\n",
    "print(\"Some sentence=\", end=\"\")\n",
    "print(some_sentence)\n",
    "transformed_sentence = getTransform(source_vocab)(engTokenize(some_sentence))\n",
    "print(\"Transformed sentence=\", end=\"\")\n",
    "print(transformed_sentence)\n",
    "index_to_string = source_vocab.get_itos()\n",
    "for index in transformed_sentence:\n",
    "    print(index_to_string[index], end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1, 618, 4, 2], [1, 750, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "def applyTransform(sequence_pair):\n",
    "    \"\"\"\n",
    "    Apply transforms to sequence of tokens in a sequence pair\n",
    "    \"\"\"\n",
    "\n",
    "    return (\n",
    "        getTransform(source_vocab)(engTokenize(sequence_pair[0])),\n",
    "        getTransform(target_vocab)(deTokenize(sequence_pair[1]))\n",
    "    )\n",
    "data_pipe = data_pipe.map(applyTransform) ## Apply the function to each element in the iterator\n",
    "temp_list = list(data_pipe)\n",
    "print(temp_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortBucket(bucket):\n",
    "    \"\"\"\n",
    "    Function to sort a given bucket. Here, we want to sort based on the length of\n",
    "    source and target sequence.\n",
    "    \"\"\"\n",
    "    return sorted(bucket, key=lambda x: (len(x[0]), len(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipe = data_pipe.bucketbatch(\n",
    "    batch_size = 4, batch_num=5,  bucket_num=1,\n",
    "    use_in_batch_shuffle=False, sort_key=sortBucket\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([1, 3, 4, 2], [1, 3850, 41, 4, 2]), ([1, 36, 23, 4, 2], [1, 2597, 18, 24, 2]), ([1, 36, 23, 4, 2], [1, 4340, 18, 4, 2]), ([1, 618, 41, 4, 2], [1, 505, 500, 4, 2])]\n"
     ]
    }
   ],
   "source": [
    "print(list(data_pipe)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(([1, 6815, 23, 10, 2], [1, 6815, 23, 10, 2], [1, 29, 472, 4, 2], [1, 29, 472, 4, 2]), ([1, 20624, 8, 2], [1, 11009, 8, 2], [1, 31, 1140, 4, 2], [1, 31, 1053, 4, 2]))\n"
     ]
    }
   ],
   "source": [
    "def separateSourceTarget(sequence_pairs):\n",
    "    \"\"\"\n",
    "    input of form: `[(X_1,y_1), (X_2,y_2), (X_3,y_3), (X_4,y_4)]`\n",
    "    output of form: `((X_1,X_2,X_3,X_4), (y_1,y_2,y_3,y_4))`\n",
    "    \"\"\"\n",
    "    sources,targets = zip(*sequence_pairs)\n",
    "    return sources,targets\n",
    "\n",
    "## Apply the function to each element in the iterator\n",
    "data_pipe = data_pipe.map(separateSourceTarget)\n",
    "print(list(data_pipe)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyPadding(pair_of_sequences):\n",
    "    \"\"\"\n",
    "    Convert sequences to tensors and apply padding\n",
    "    \"\"\"\n",
    "    return (T.ToTensor(0)(list(pair_of_sequences[0])), T.ToTensor(0)(list(pair_of_sequences[1])))\n",
    "## `T.ToTensor(0)` returns a transform that converts the sequence to `torch.tensor` and also applies\n",
    "# padding. Here, `0` is passed to the constructor to specify the index of the `<pad>` token in the\n",
    "# vocabulary.\n",
    "data_pipe = data_pipe.map(applyPadding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
